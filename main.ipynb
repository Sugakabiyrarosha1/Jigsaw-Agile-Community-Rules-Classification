{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fd7a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Jigsaw — SCRATCH Hyperparameter Tuning (Grid/Random, Resumable)\n",
    "# * Windows/VS Code/Jupyter safe DataLoader (num_workers=0 on Windows)\n",
    "# * tqdm console progress bars (no ipywidgets errors)\n",
    "# * Early stopping\n",
    "# * TensorBoard logging (train loss, val AUC/ACC, hparams snapshot)\n",
    "# * Skips combos already logged in trial_results_scratch.csv\n",
    "# * Optional per-trial checkpoints\n",
    "# * Best trial of the session writes submission_scratch.(csv|xlsx)\n",
    "# ============================================================\n",
    "\n",
    "import os, re, json, time, random, hashlib, platform\n",
    "from datetime import datetime, timezone\n",
    "from itertools import product\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "# ------------------- Paths & switches -------------------\n",
    "TRAIN_PATH = \"train.csv\"\n",
    "TEST_PATH  = \"test.csv\"\n",
    "SUB_PATH   = \"sample_submission.csv\"\n",
    "\n",
    "RESULTS_CSV = \"trial_results_scratch.csv\"    # resumable log (append)\n",
    "CHECKPOINT_DIR = \"checkpoints_scratch\"       # per-trial .pt files\n",
    "SAVE_CHECKPOINTS = True\n",
    "\n",
    "# TensorBoard\n",
    "ENABLE_TENSORBOARD = True                    # turn on/off\n",
    "TB_LOGDIR_BASE = \"tb_scratch_tune\"           # tensorboard --logdir tb_scratch_tune\n",
    "TB_WRITE_HPARAMS = True\n",
    "\n",
    "EARLY_STOP_PATIENCE = 3                      # epochs without AUC gain before stopping (0 to disable)\n",
    "MAX_TRIALS_PER_RUN  = 10                     # safety cap per session\n",
    "SAVE_BEST_SESSION_SUBMISSION = True\n",
    "SAVE_EVERY_TRIAL_SUBMISSION = True  # NEW: Save submission after every trial\n",
    "SUBMISSION_CSV  = \"submission_scratch.csv\"\n",
    "SUBMISSION_XLSX = \"submission_scratch.xlsx\"\n",
    "\n",
    "# --- IO / dataloader runtime safety (Windows/Jupyter safe) ---\n",
    "IS_WINDOWS = (os.name == \"nt\")\n",
    "NUM_WORKERS = 0 if IS_WINDOWS else 2         # KEY: avoid multiprocessing on Windows\n",
    "PERSISTENT_WORKERS = False\n",
    "PIN_MEMORY = torch.cuda.is_available()\n",
    "LOG_EVERY_N = 50                              # fallback batch logging if tqdm unavailable\n",
    "\n",
    "# Progress bars: force console (no ipywidgets)\n",
    "FORCE_CONSOLE_TQDM = True\n",
    "if FORCE_CONSOLE_TQDM:\n",
    "    os.environ[\"TQDM_NOTEBOOK\"] = \"0\"\n",
    "    try:\n",
    "        from tqdm import tqdm  # console bar\n",
    "    except Exception:\n",
    "        tqdm = None\n",
    "else:\n",
    "    tqdm = None\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# ------------------- Load data -------------------\n",
    "assert os.path.exists(TRAIN_PATH) and os.path.exists(TEST_PATH) and os.path.exists(SUB_PATH), \\\n",
    "    \"Place train.csv, test.csv, sample_submission.csv in the working directory.\"\n",
    "\n",
    "TEXT_COLS = ['body','rule','subreddit','positive_example_1','positive_example_2','negative_example_1','negative_example_2']\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df  = pd.read_csv(TEST_PATH)\n",
    "\n",
    "for df in [train_df, test_df]:\n",
    "    for c in TEXT_COLS:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "def build_input_template(row):\n",
    "    return \" [SEP] \".join([\n",
    "        f\"[COMMENT] {row['body']}\",\n",
    "        f\"[RULE] {row['rule']}\",\n",
    "        f\"[POS_EX_1] {row['positive_example_1']}\",\n",
    "        f\"[POS_EX_2] {row['positive_example_2']}\",\n",
    "        f\"[NEG_EX_1] {row['negative_example_1']}\",\n",
    "        f\"[NEG_EX_2] {row['negative_example_2']}\",\n",
    "        f\"[SUBREDDIT] r/{row['subreddit']}\"\n",
    "    ])\n",
    "\n",
    "if \"input_text\" not in train_df.columns:\n",
    "    train_df[\"input_text\"] = train_df.apply(build_input_template, axis=1)\n",
    "    test_df[\"input_text\"]  = test_df.apply(build_input_template, axis=1)\n",
    "\n",
    "# ------------------- Utils -------------------\n",
    "def set_seed(seed:int=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def now_iso(): return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def combo_key(params:Dict[str,Any])->str:\n",
    "    s = json.dumps({k:params[k] for k in sorted(params)}, sort_keys=True)\n",
    "    return hashlib.md5(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def load_done_keys(path:str)->set:\n",
    "    if not os.path.exists(path): return set()\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        return set(df[\"key\"].astype(str).tolist()) if \"key\" in df.columns else set()\n",
    "    except Exception:\n",
    "        return set()\n",
    "\n",
    "def append_result_row(row:Dict[str,Any], path=RESULTS_CSV):\n",
    "    df = pd.DataFrame([row], columns=list(row.keys()))\n",
    "    if os.path.exists(path): df.to_csv(path, mode=\"a\", header=False, index=False)\n",
    "    else:                    df.to_csv(path, index=False)\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(TB_LOGDIR_BASE, exist_ok=True)\n",
    "\n",
    "# ------------------- Tokenizer/Vocab -------------------\n",
    "TOKEN_RE = re.compile(r\"[A-Za-z0-9_']+\")\n",
    "def tokenize(s): return TOKEN_RE.findall((s or \"\").lower())\n",
    "\n",
    "VOCAB_CACHE: Dict[int, Dict[str,int]] = {}\n",
    "def build_vocab(df:pd.DataFrame, vocab_size:int=30000)->Dict[str,int]:\n",
    "    if vocab_size in VOCAB_CACHE: return VOCAB_CACHE[vocab_size]\n",
    "    from collections import Counter\n",
    "    cnt = Counter()\n",
    "    for col in [\"body\",\"rule\"]:\n",
    "        for txt in df[col].tolist():\n",
    "            cnt.update(tokenize(txt))\n",
    "    vocab = {\"<pad>\":0, \"<unk>\":1}\n",
    "    for i,(tok,_) in enumerate(cnt.most_common(vocab_size-2), start=2):\n",
    "        vocab[tok] = i\n",
    "    VOCAB_CACHE[vocab_size] = vocab\n",
    "    return vocab\n",
    "\n",
    "def encode_text(s, vocab, max_len):\n",
    "    ids = [vocab.get(t,1) for t in tokenize(s)][:max_len]\n",
    "    if len(ids) < max_len: ids += [0]*(max_len-len(ids))\n",
    "    return np.array(ids, dtype=np.int64)\n",
    "\n",
    "class ScratchDataset(Dataset):\n",
    "    def __init__(self, df, vocab, seq_len, with_labels=True):\n",
    "        self.df=df.reset_index(drop=True); self.vocab=vocab; self.seq_len=seq_len; self.with_labels=with_labels\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, i):\n",
    "        r = self.df.loc[i]\n",
    "        half = self.seq_len//2\n",
    "        x = np.concatenate([encode_text(r[\"body\"], self.vocab, half),\n",
    "                            encode_text(r[\"rule\"], self.vocab, half)])\n",
    "        if self.with_labels:\n",
    "            y = int(r[\"rule_violation\"])\n",
    "            return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.float32)\n",
    "        return torch.tensor(x, dtype=torch.long)\n",
    "\n",
    "def make_dataloader(ds: Dataset, batch_size: int, shuffle: bool) -> DataLoader:\n",
    "    kwargs = dict(batch_size=batch_size, shuffle=shuffle, num_workers=NUM_WORKERS)\n",
    "    if NUM_WORKERS > 0:\n",
    "        kwargs[\"prefetch_factor\"] = 2\n",
    "        kwargs[\"persistent_workers\"] = PERSISTENT_WORKERS\n",
    "    if torch.cuda.is_available():\n",
    "        kwargs[\"pin_memory\"] = PIN_MEMORY\n",
    "    return DataLoader(ds, **kwargs)\n",
    "\n",
    "# ------------------- Model -------------------\n",
    "def parse_kernel_sizes(spec:str):\n",
    "    ks = []\n",
    "    for k in str(spec).split(\"-\"):\n",
    "        k = k.strip()\n",
    "        if k.isdigit(): ks.append(int(k))\n",
    "    return ks or [3,5]\n",
    "\n",
    "def channel_schedule(start:int, blocks:int, growth:str):\n",
    "    chs = [start]\n",
    "    for _ in range(1, blocks):\n",
    "        if growth == \"x1.5\": chs.append(int(round(chs[-1]*1.5)))\n",
    "        elif growth == \"x2\": chs.append(chs[-1]*2)\n",
    "        else:                chs.append(chs[-1])\n",
    "    return chs\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, conv_blocks, channels_start,\n",
    "                 channel_growth, kernel_sizes_spec, use_batchnorm=True,\n",
    "                 pooling=\"max\", dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        ks = parse_kernel_sizes(kernel_sizes_spec)\n",
    "        chs = channel_schedule(channels_start, conv_blocks, channel_growth)\n",
    "        self.blocks = nn.ModuleList()\n",
    "        in_ch = emb_dim\n",
    "        for bi in range(conv_blocks):\n",
    "            k = ks[min(bi, len(ks)-1)]\n",
    "            out_ch = chs[bi]\n",
    "            conv = nn.Conv1d(in_ch, out_ch, kernel_size=k, padding=k//2)\n",
    "            bn   = nn.BatchNorm1d(out_ch) if use_batchnorm else nn.Identity()\n",
    "            self.blocks.append(nn.Sequential(conv, bn, nn.ReLU()))\n",
    "            in_ch = out_ch\n",
    "        self.pooling = pooling\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(in_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e = self.emb(x).transpose(1,2)   # [B,E,L]\n",
    "        h = e\n",
    "        for blk in self.blocks: h = blk(h)\n",
    "        if self.pooling == \"avg\": h = F.adaptive_avg_pool1d(h,1).squeeze(-1)\n",
    "        else:                     h = F.adaptive_max_pool1d(h,1).squeeze(-1)\n",
    "        h = self.drop(h)\n",
    "        return self.fc(h).squeeze(-1)\n",
    "\n",
    "# ------------------- Loss/Optim/Val -------------------\n",
    "class BCEWithLS(nn.Module):\n",
    "    def __init__(self, smoothing=0.0): super().__init__(); self.s=smoothing\n",
    "    def forward(self, logits, targets):\n",
    "        if self.s>0: targets = targets*(1-self.s)+0.5*self.s\n",
    "        return F.binary_cross_entropy_with_logits(logits, targets)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, smoothing=0.0): super().__init__(); self.g=gamma; self.s=smoothing\n",
    "    def forward(self, logits, targets):\n",
    "        p = torch.sigmoid(logits)\n",
    "        if self.s>0: targets = targets*(1-self.s)+0.5*self.s\n",
    "        loss_pos = -targets * ((1-p)**self.g) * torch.log(torch.clamp(p, 1e-8, 1.0))\n",
    "        loss_neg = -(1-targets) * (p**self.g) * torch.log(torch.clamp(1-p, 1.0-1e-8))\n",
    "        return (loss_pos+loss_neg).mean()\n",
    "\n",
    "def get_loss(name, smoothing):\n",
    "    return FocalLoss(2.0, smoothing) if name==\"focal\" else BCEWithLS(smoothing)\n",
    "\n",
    "def make_optimizer(model, name, lr, weight_decay):\n",
    "    if name == \"adamw\": return torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif name == \"sgd\": return torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "    else: raise ValueError(f\"Unknown optimizer: {name}\")\n",
    "\n",
    "def _epoch_validate(model, dl, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    preds, ys = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb,yb in dl:\n",
    "            xb,yb = xb.to(device), yb.to(device)\n",
    "            p = torch.sigmoid(model(xb)).detach().cpu().numpy()\n",
    "            preds.append(p); ys.append(yb.detach().cpu().numpy())\n",
    "    preds = np.concatenate(preds); ys = np.concatenate(ys)\n",
    "    auc = roc_auc_score(ys, preds)\n",
    "    acc = accuracy_score(ys.astype(int), (preds >= 0.5).astype(int))\n",
    "    return auc, acc, preds, ys\n",
    "\n",
    "# ------------------- Train one combo -------------------\n",
    "def train_eval_once_with_best(params:dict, enable_tb:bool=False):\n",
    "    set_seed(int(params[\"seed\"]))\n",
    "    vocab = build_vocab(train_df, int(params[\"vocab_size\"]))\n",
    "    seq_len = int(params[\"seq_len\"])\n",
    "    tr, va = train_test_split(train_df, test_size=0.2, random_state=int(params[\"seed\"]),\n",
    "                              stratify=train_df[\"rule_violation\"])\n",
    "    ds_tr = ScratchDataset(tr, vocab, seq_len, True)\n",
    "    ds_va = ScratchDataset(va, vocab, seq_len, True)\n",
    "    dl_tr = make_dataloader(ds_tr, int(params[\"batch_size\"]), True)\n",
    "    dl_va = make_dataloader(ds_va, int(params[\"batch_size\"]), False)\n",
    "\n",
    "    model = TextCNN(\n",
    "        vocab_size=len(vocab),\n",
    "        emb_dim=int(params[\"emb_dim\"]),\n",
    "        conv_blocks=int(params[\"conv_blocks\"]),\n",
    "        channels_start=int(params[\"channels_start\"]),\n",
    "        channel_growth=str(params[\"channel_growth\"]),\n",
    "        kernel_sizes_spec=str(params[\"kernel_sizes\"]),\n",
    "        use_batchnorm=bool(params[\"use_batchnorm\"]),\n",
    "        pooling=str(params[\"pooling\"]),\n",
    "        dropout=float(params[\"dropout\"])\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    opt = make_optimizer(model, str(params[\"optimizer\"]), float(params[\"learning_rate\"]), float(params[\"weight_decay\"]))\n",
    "    loss_fn = get_loss(str(params[\"loss_fn\"]), float(params[\"label_smoothing\"]))\n",
    "    grad_clip = float(params[\"grad_clip\"])\n",
    "    epochs = int(params[\"epochs\"])\n",
    "\n",
    "    pos_weight = None\n",
    "    if str(params[\"class_weighting\"])==\"balanced\":\n",
    "        pos_weight = torch.tensor([(len(tr)-tr[\"rule_violation\"].sum())/(tr[\"rule_violation\"].sum()+1e-6)], device=DEVICE)\n",
    "\n",
    "    tb = None\n",
    "    tb_run_dir = None\n",
    "    if enable_tb:\n",
    "        try:\n",
    "            from torch.utils.tensorboard import SummaryWriter\n",
    "            tag = (\n",
    "                f\"emb{params['emb_dim']}_cb{params['conv_blocks']}_ch{params['channels_start']}\"\n",
    "                f\"_lr{params['learning_rate']}_bs{params['batch_size']}\"\n",
    "            )\n",
    "            tb_run_dir = os.path.join(TB_LOGDIR_BASE, f\"{tag}_{datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%S')}\")\n",
    "            tb = SummaryWriter(log_dir=tb_run_dir)\n",
    "            tb.add_text(\"hparams/json\", json.dumps(params, indent=2))\n",
    "        except Exception as e:\n",
    "            print(\"TensorBoard unavailable:\", e)\n",
    "            tb = None\n",
    "\n",
    "    best_auc, best_acc, best_state = -1.0, 0.0, None\n",
    "    global_step = 0\n",
    "    no_improve = 0\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        iterator = dl_tr if tqdm is None else tqdm(dl_tr, leave=False, desc=f\"Epoch {ep+1}/{epochs}\")\n",
    "        for i, (xb, yb) in enumerate(iterator):\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = (F.binary_cross_entropy_with_logits(logits, yb, pos_weight=pos_weight)\n",
    "                    if pos_weight is not None else loss_fn(logits, yb))\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            opt.step()\n",
    "            if tqdm is None and (i % LOG_EVERY_N == 0):\n",
    "                print(f\"  batch {i:>4}/{len(dl_tr)}  loss={float(loss.item()):.4f}\")\n",
    "            if tb:\n",
    "                tb.add_scalar(\"train/loss\", float(loss.item()), global_step)\n",
    "            global_step += 1\n",
    "\n",
    "        auc, acc, _, _ = _epoch_validate(model, dl_va, device=DEVICE)\n",
    "        improved = auc > best_auc + 1e-5\n",
    "        if improved:\n",
    "            best_auc, best_acc = auc, acc\n",
    "            best_state = {k: v.detach().cpu() for k,v in model.state_dict().items()}\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        print(f\"[SCRATCH] Epoch {ep+1}/{epochs} AUC={auc:.5f} ACC={acc:.4f} \"\n",
    "              f\"(best {best_auc:.5f}, patience {no_improve}/{EARLY_STOP_PATIENCE})\")\n",
    "        if tb:\n",
    "            tb.add_scalar(\"val/auc\", float(auc), ep)\n",
    "            tb.add_scalar(\"val/accuracy\", float(acc), ep)\n",
    "\n",
    "        if EARLY_STOP_PATIENCE and no_improve >= EARLY_STOP_PATIENCE:\n",
    "            print(\"Early stopping: no improvement.\")\n",
    "            break\n",
    "\n",
    "    if tb:\n",
    "        # Snapshot final best metrics + (optional) hparams summary\n",
    "        tb.add_scalar(\"val/best_auc\", float(best_auc))\n",
    "        tb.add_scalar(\"val/best_acc\", float(best_acc))\n",
    "        if TB_WRITE_HPARAMS:\n",
    "            try:\n",
    "                # TensorBoard HParams (writes to a separate event file inside this run)\n",
    "                from torch.utils.tensorboard.summary import hparams\n",
    "                metric_dict = {\"hparam/best_auc\": float(best_auc), \"hparam/best_acc\": float(best_acc)}\n",
    "                tb.file_writer.add_summary(hparams(params, metric_dict))\n",
    "            except Exception:\n",
    "                pass\n",
    "        tb.close()\n",
    "\n",
    "    return best_auc, best_acc, best_state, vocab\n",
    "\n",
    "# ------------------- Predict test with a state -------------------\n",
    "def predict_test_with_state(best_state, params, vocab, out_csv=\"submission_scratch.csv\"):\n",
    "    seq_len = int(params[\"seq_len\"])\n",
    "    class TestDS(Dataset):\n",
    "        def __init__(self, df, vocab, seq_len):\n",
    "            self.df=df.reset_index(drop=True); self.vocab=vocab; self.seq_len=seq_len\n",
    "        def __len__(self): return len(self.df)\n",
    "        def __getitem__(self, i):\n",
    "            r = self.df.loc[i]\n",
    "            half = self.seq_len//2\n",
    "            x = np.concatenate([encode_text(r[\"body\"], self.vocab, half),\n",
    "                                encode_text(r[\"rule\"], self.vocab, half)])\n",
    "            return torch.tensor(x, dtype=torch.long)\n",
    "\n",
    "    test_ds = TestDS(test_df, vocab, seq_len)\n",
    "    test_dl = make_dataloader(test_ds, int(params[\"batch_size\"]), False)\n",
    "\n",
    "    model = TextCNN(\n",
    "        vocab_size=len(vocab),\n",
    "        emb_dim=int(params[\"emb_dim\"]),\n",
    "        conv_blocks=int(params[\"conv_blocks\"]),\n",
    "        channels_start=int(params[\"channels_start\"]),\n",
    "        channel_growth=str(params[\"channel_growth\"]),\n",
    "        kernel_sizes_spec=str(params[\"kernel_sizes\"]),\n",
    "        use_batchnorm=bool(params[\"use_batchnorm\"]),\n",
    "        pooling=str(params[\"pooling\"]),\n",
    "        dropout=float(params[\"dropout\"])\n",
    "    ).to(DEVICE)\n",
    "    model.load_state_dict({k: v.to(DEVICE) for k,v in best_state.items()})\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for xb in test_dl:\n",
    "            xb = xb.to(DEVICE)\n",
    "            p = torch.sigmoid(model(xb)).detach().cpu().numpy()\n",
    "            preds.append(p)\n",
    "    preds = np.concatenate(preds).reshape(-1)\n",
    "\n",
    "    sub = pd.read_csv(SUB_PATH).copy()\n",
    "    if \"row_id\" not in sub.columns:\n",
    "        if \"row_id\" in test_df.columns:\n",
    "            sub = test_df[[\"row_id\"]].copy()\n",
    "        else:\n",
    "            sub[\"row_id\"] = np.arange(len(preds))\n",
    "    sub[\"rule_violation\"] = np.clip(preds, 0, 1)\n",
    "    sub.to_csv(out_csv, index=False)\n",
    "    print(f\"✅ Wrote {out_csv} (rows={len(sub)})\")\n",
    "    return out_csv\n",
    "\n",
    "# ------------------- Param space handling -------------------\n",
    "CONSTANTS_DEFAULT = {\n",
    "    \"vocab_size\": 30000,\n",
    "    \"use_batchnorm\": True,\n",
    "    \"pooling\": \"max\",\n",
    "    \"optimizer\": \"adamw\",\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"scheduler\": \"none\",        # catalog only\n",
    "    \"class_weighting\": \"none\",\n",
    "    \"seed\": 42,\n",
    "}\n",
    "REQ = ['seq_len','emb_dim','conv_blocks','channels_start','channel_growth','kernel_sizes',\n",
    "       'dropout','weight_decay','label_smoothing','learning_rate','batch_size','epochs','loss_fn']\n",
    "INTS   = [\"seq_len\",\"emb_dim\",\"conv_blocks\",\"channels_start\",\"batch_size\",\"epochs\",\"seed\"]\n",
    "FLOATS = [\"dropout\",\"weight_decay\",\"label_smoothing\",\"learning_rate\",\"grad_clip\"]\n",
    "STRS   = [\"channel_growth\",\"kernel_sizes\",\"pooling\",\"optimizer\",\"scheduler\",\"loss_fn\",\"class_weighting\"]\n",
    "BOOLS  = [\"use_batchnorm\"]\n",
    "\n",
    "def coerce_one(p:Dict[str,Any])->Dict[str,Any]:\n",
    "    x = {**CONSTANTS_DEFAULT, **p}\n",
    "    missing = [k for k in REQ if k not in x]\n",
    "    if missing: raise KeyError(f\"Missing required param(s): {missing}\")\n",
    "    for k in INTS:   x[k] = int(x[k])\n",
    "    for k in FLOATS: x[k] = float(x[k])\n",
    "    for k in STRS:   x[k] = str(x[k])\n",
    "    for k in BOOLS:\n",
    "        v = x[k]; x[k] = (v.strip().lower() in (\"true\",\"1\",\"yes\",\"y\")) if isinstance(v,str) else bool(v)\n",
    "    return x\n",
    "\n",
    "def expand_grid(space:Dict[str,List[Any]], shuffle=True, seed=42)->List[Dict[str,Any]]:\n",
    "    from itertools import product\n",
    "    keys = list(space.keys())\n",
    "    vals = [space[k] if isinstance(space[k], (list, tuple)) else [space[k]] for k in keys]\n",
    "    combos = []\n",
    "    for tup in product(*vals):\n",
    "        combos.append({k:v for k,v in zip(keys, tup)})\n",
    "    if shuffle:\n",
    "        rnd = random.Random(seed); rnd.shuffle(combos)\n",
    "    return combos\n",
    "\n",
    "# ------------------- Tuner (grid/random + resume) -------------------\n",
    "def run_param_space(space:Dict[str,List[Any]],\n",
    "                    constants:Dict[str,Any]=None,\n",
    "                    mode:str=\"grid\",        # \"grid\" or \"random\"\n",
    "                    n_samples:int=None,     # only for mode=\"random\"\n",
    "                    max_trials:int=10,\n",
    "                    enable_tb:bool=False,\n",
    "                    save_best_submission:bool=True):\n",
    "    constants = constants or {}\n",
    "    grid = expand_grid(space, shuffle=True, seed=int(constants.get(\"seed\", 42)))\n",
    "    if mode == \"random\" and n_samples is not None:\n",
    "        grid = grid[:n_samples]  # shuffled already\n",
    "\n",
    "    done = load_done_keys(RESULTS_CSV)\n",
    "    print(f\"Total combos: {len(grid)} | Completed in CSV: {len(done)}\")\n",
    "\n",
    "    best_auc = -1.0\n",
    "    best_payload = None\n",
    "    ran = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    for idx, raw in enumerate(grid):\n",
    "        params = coerce_one({**raw, **constants})\n",
    "        key = combo_key(params)\n",
    "        if key in done:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n=== Trial {ran+1}/{max_trials} | idx={idx} ===\")\n",
    "        print({k: params[k] for k in REQ})\n",
    "\n",
    "        t1 = time.time()\n",
    "        try:\n",
    "            auc, acc, state, vocab = train_eval_once_with_best(params, enable_tb)\n",
    "            status = \"ok\"\n",
    "            if SAVE_CHECKPOINTS and state is not None:\n",
    "                torch.save({\"state_dict\": state, \"params\": params},\n",
    "                           os.path.join(CHECKPOINT_DIR, f\"{key}.pt\"))\n",
    "        except Exception as e:\n",
    "            auc, acc = float(\"nan\"), float(\"nan\")\n",
    "            state, vocab = None, None\n",
    "            status = f\"error: {e}\"\n",
    "            print(\"❌\", e)\n",
    "        dur = time.time() - t1\n",
    "\n",
    "        row_out = {\n",
    "            \"timestamp\": now_iso(),\n",
    "            \"key\": key,\n",
    "            \"mode\": \"scratch\",\n",
    "            \"device\": DEVICE,\n",
    "            \"python\": platform.python_version(),\n",
    "            \"grid_idx\": idx,\n",
    "            \"val_auc\": auc,\n",
    "            \"val_acc\": acc,\n",
    "            \"runtime_sec\": round(dur,2),\n",
    "            \"status\": status,\n",
    "            **{f\"hp/{k}\": params[k] for k in sorted(params)}\n",
    "        }\n",
    "        append_result_row(row_out, RESULTS_CSV)\n",
    "        ran += 1\n",
    "\n",
    "        if status == \"ok\" and auc > best_auc:\n",
    "            best_auc = auc\n",
    "            best_payload = (state, params, vocab)\n",
    "            \n",
    "            # Save submission after every successful trial (if enabled)\n",
    "            if SAVE_EVERY_TRIAL_SUBMISSION:\n",
    "                trial_submission_csv = f\"submission_trial_{ran:03d}_auc_{auc:.4f}.csv\"\n",
    "                trial_submission_xlsx = f\"submission_trial_{ran:03d}_auc_{auc:.4f}.xlsx\"\n",
    "                \n",
    "                predict_test_with_state(state, params, vocab, out_csv=trial_submission_csv)\n",
    "                try:\n",
    "                    sub_df = pd.read_csv(trial_submission_csv)\n",
    "                    with pd.ExcelWriter(trial_submission_xlsx, engine=\"xlsxwriter\") as w:\n",
    "                        sub_df.to_excel(w, sheet_name=\"submission\", index=False)\n",
    "                    print(f\"✅ Wrote trial submission: {trial_submission_csv} and {trial_submission_xlsx}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Note: could not write XLSX for trial {ran}:\", e)\n",
    "\n",
    "        if ran >= max_trials:\n",
    "            break\n",
    "\n",
    "    print(f\"\\nSession done. Ran {ran} trial(s) in {round(time.time()-t0,2)}s.\")\n",
    "    if best_payload and save_best_submission:\n",
    "        state, params, vocab = best_payload\n",
    "        predict_test_with_state(state, params, vocab, out_csv=SUBMISSION_CSV)\n",
    "        try:\n",
    "            sub_df = pd.read_csv(SUBMISSION_CSV)\n",
    "            with pd.ExcelWriter(SUBMISSION_XLSX, engine=\"xlsxwriter\") as w:\n",
    "                sub_df.to_excel(w, sheet_name=\"submission\", index=False)\n",
    "            print(f\"✅ Wrote {SUBMISSION_XLSX}\")\n",
    "        except Exception as e:\n",
    "            print(\"Note: could not write XLSX submission:\", e)\n",
    "    else:\n",
    "        print(\"No submission written this session.\")\n",
    "\n",
    "# ============================================================\n",
    "# DEFINE YOUR PARAM SPACE HERE (laptop-safe; resume lets you add more)\n",
    "# \n",
    "# HOW TO ADD MORE PARAMETERS:\n",
    "# 1. Add new parameter to PARAM_SPACE with a list of values to try\n",
    "# 2. Update your model/training code to use the new parameter\n",
    "# 3. Add parameter to REQ list if it's required for model creation\n",
    "# 4. The system will automatically generate all combinations\n",
    "#\n",
    "# EXAMPLES:\n",
    "# - Add new optimizers: optimizer=[\"adam\", \"adamw\", \"sgd\", \"rmsprop\"]\n",
    "# - Add new architectures: model_type=[\"cnn\", \"transformer\", \"lstm\"]\n",
    "# - Add new data augmentation: augmentation=[\"none\", \"backtranslation\", \"paraphrase\"]\n",
    "# ============================================================\n",
    "PARAM_SPACE = dict(\n",
    "    # Capacity/structure\n",
    "    seq_len=[200, 224, 256, 288, 320],  # Expanded sequence lengths\n",
    "    emb_dim=[96, 128, 160, 192, 224],   # More embedding dimensions\n",
    "    conv_blocks=[1, 2, 3],              # More convolution blocks\n",
    "    channels_start=[96, 128, 160, 192], # More starting channels\n",
    "    channel_growth=[\"x1.2\", \"x1.5\", \"x2.0\"],  # Different growth rates\n",
    "    kernel_sizes=[\"3-5-7\", \"3-5-7-9\", \"5-7-9\"],  # More kernel size combinations\n",
    "\n",
    "    # Optimization/regularization\n",
    "    optimizer=[\"adam\", \"adamw\", \"sgd\"],  # More optimizers\n",
    "    learning_rate=[5e-4, 8e-4, 1e-3, 1.2e-3, 1.5e-3],  # More learning rates\n",
    "    batch_size=[32, 64, 128, 256],       # More batch sizes\n",
    "    epochs=[6, 8, 10, 12],               # More epoch options\n",
    "    dropout=[0.1, 0.15, 0.2, 0.25, 0.3], # More dropout rates\n",
    "    weight_decay=[0, 1e-5, 1e-4, 2e-4, 5e-4],  # More weight decay options\n",
    "    label_smoothing=[0.0, 0.01, 0.03, 0.05],   # More label smoothing options\n",
    "    loss_fn=[\"bce_logits\", \"focal_loss\"],       # More loss functions\n",
    "    \n",
    "    # NEW: Additional hyperparameters\n",
    "    warmup_epochs=[0, 1, 2],             # Learning rate warmup\n",
    "    scheduler_type=[\"none\", \"cosine\", \"step\", \"plateau\"],  # Learning rate schedulers\n",
    "    grad_clip=[0.5, 1.0, 1.5, 2.0],     # Gradient clipping values\n",
    "    activation=[\"relu\", \"gelu\", \"swish\"], # Activation functions\n",
    "    pooling_type=[\"max\", \"avg\", \"attention\"],  # Pooling methods\n",
    "    use_batchnorm=[True, False],         # Batch normalization toggle\n",
    "    use_residual=[True, False],          # Residual connections\n",
    "    attention_heads=[1, 2, 4],           # Multi-head attention (if using attention pooling)\n",
    ")\n",
    "\n",
    "# Constants applied to every combo (change here if needed)\n",
    "CONSTANTS = dict(\n",
    "    vocab_size=30000,\n",
    "    class_weighting=\"none\",\n",
    "    seed=42,\n",
    "    # Note: use_batchnorm, pooling, grad_clip, scheduler are now in PARAM_SPACE\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# GO: run grid (or random sample) with resume\n",
    "# Start TensorBoard in a terminal:  tensorboard --logdir tb_scratch_tune\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    run_param_space(\n",
    "        PARAM_SPACE,\n",
    "        constants=CONSTANTS,\n",
    "        mode=\"grid\",             # or \"random\"\n",
    "        n_samples=None,          # only used for mode=\"random\"\n",
    "        max_trials=200, #MAX_TRIALS_PER_RUN\n",
    "        enable_tb=ENABLE_TENSORBOARD,\n",
    "        save_best_submission=SAVE_BEST_SESSION_SUBMISSION\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204a532d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Total combos: 6912 | Completed in CSV: 0\n",
      "\n",
      "=== Trial 1/6912 | idx=0 ===\n",
      "{'seq_len': 256, 'emb_dim': 192, 'conv_blocks': 2, 'channels_start': 128, 'channel_growth': 'x1.5', 'kernel_sizes': '3-5-7', 'dropout': 0.25, 'weight_decay': 0.0002, 'label_smoothing': 0.0, 'learning_rate': 0.0008, 'batch_size': 128, 'epochs': 8, 'loss_fn': 'bce_logits'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCRATCH] Epoch 1/8 | train_loss=1.4572 train_acc=0.5410 val_auc=0.72612 val_acc=0.6601 (best 0.72612, patience 0/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCRATCH] Epoch 2/8 | train_loss=0.7424 train_acc=0.6531 val_auc=0.78167 val_acc=0.6897 (best 0.78167, patience 0/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCRATCH] Epoch 3/8 | train_loss=0.5147 train_acc=0.7505 val_auc=0.76682 val_acc=0.6798 (best 0.78167, patience 1/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCRATCH] Epoch 4/8 | train_loss=0.3598 train_acc=0.8410 val_auc=0.77058 val_acc=0.6921 (best 0.78167, patience 2/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCRATCH] Epoch 5/8 | train_loss=0.2432 train_acc=0.9150 val_auc=0.75811 val_acc=0.6478 (best 0.78167, patience 3/3)\n",
      "Early stopping: no improvement.\n",
      "\n",
      "=== Trial 2/6912 | idx=1 ===\n",
      "{'seq_len': 200, 'emb_dim': 160, 'conv_blocks': 1, 'channels_start': 128, 'channel_growth': 'x1.5', 'kernel_sizes': '3-5-7', 'dropout': 0.2, 'weight_decay': 0.0002, 'label_smoothing': 0.03, 'learning_rate': 0.0008, 'batch_size': 128, 'epochs': 8, 'loss_fn': 'bce_logits'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCRATCH] Epoch 1/8 | train_loss=0.8820 train_acc=0.5551 val_auc=0.72709 val_acc=0.6700 (best 0.72709, patience 0/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCRATCH] Epoch 2/8 | train_loss=0.6301 train_acc=0.6802 val_auc=0.75141 val_acc=0.6724 (best 0.75141, patience 0/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCRATCH] Epoch 3/8 | train_loss=0.5314 train_acc=0.7461 val_auc=0.76806 val_acc=0.6946 (best 0.76806, patience 0/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCRATCH] Epoch 4/8 | train_loss=0.4442 train_acc=0.8084 val_auc=0.77388 val_acc=0.6650 (best 0.77388, patience 0/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCRATCH] Epoch 5/8 | train_loss=0.3813 train_acc=0.8503 val_auc=0.77672 val_acc=0.6995 (best 0.77672, patience 0/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCRATCH] Epoch 6/8 | train_loss=0.3395 train_acc=0.8749 val_auc=0.78512 val_acc=0.7167 (best 0.78512, patience 0/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCRATCH] Epoch 7/8 | train_loss=0.3141 train_acc=0.8916 val_auc=0.78541 val_acc=0.7217 (best 0.78541, patience 0/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCRATCH] Epoch 8/8 | train_loss=0.2694 train_acc=0.9193 val_auc=0.77633 val_acc=0.7118 (best 0.78541, patience 1/3)\n",
      "\n",
      "=== Trial 3/6912 | idx=2 ===\n",
      "{'seq_len': 200, 'emb_dim': 128, 'conv_blocks': 1, 'channels_start': 160, 'channel_growth': 'x1.5', 'kernel_sizes': '3-5-7', 'dropout': 0.25, 'weight_decay': 0.0002, 'label_smoothing': 0.03, 'learning_rate': 0.0012, 'batch_size': 64, 'epochs': 8, 'loss_fn': 'bce_logits'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCRATCH] Epoch 1/8 | train_loss=0.9469 train_acc=0.5533 val_auc=0.78714 val_acc=0.6921 (best 0.78714, patience 0/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCRATCH] Epoch 2/8 | train_loss=0.6239 train_acc=0.7018 val_auc=0.79107 val_acc=0.6995 (best 0.79107, patience 0/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCRATCH] Epoch 3/8 | train_loss=0.4577 train_acc=0.8022 val_auc=0.78871 val_acc=0.7069 (best 0.79107, patience 1/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCRATCH] Epoch 4/8 | train_loss=0.3945 train_acc=0.8355 val_auc=0.80396 val_acc=0.7167 (best 0.80396, patience 0/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCRATCH] Epoch 5/8 | train_loss=0.3313 train_acc=0.8799 val_auc=0.80150 val_acc=0.7167 (best 0.80396, patience 1/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCRATCH] Epoch 6/8 | train_loss=0.2911 train_acc=0.9008 val_auc=0.80505 val_acc=0.7217 (best 0.80505, patience 0/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Trial interrupted by user. Saved heartbeat & latest checkpoint.\n",
      "\n",
      "=== Trial 4/6912 | idx=3 ===\n",
      "{'seq_len': 200, 'emb_dim': 192, 'conv_blocks': 2, 'channels_start': 160, 'channel_growth': 'x1.5', 'kernel_sizes': '3-5-7', 'dropout': 0.2, 'weight_decay': 0.0001, 'label_smoothing': 0.03, 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 8, 'loss_fn': 'bce_logits'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCRATCH] Epoch 1/8 | train_loss=1.3909 train_acc=0.5391 val_auc=0.72277 val_acc=0.6404 (best 0.72277, patience 0/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCRATCH] Epoch 2/8 | train_loss=0.5998 train_acc=0.6827 val_auc=0.76495 val_acc=0.6601 (best 0.76495, patience 0/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCRATCH] Epoch 3/8 | train_loss=0.4539 train_acc=0.8016 val_auc=0.77041 val_acc=0.6675 (best 0.77041, patience 0/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCRATCH] Epoch 4/8 | train_loss=0.3194 train_acc=0.8934 val_auc=0.75968 val_acc=0.7020 (best 0.77041, patience 1/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCRATCH] Epoch 5/8 | train_loss=0.2302 train_acc=0.9421 val_auc=0.76248 val_acc=0.6946 (best 0.77041, patience 2/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCRATCH] Epoch 6/8 | train_loss=0.2020 train_acc=0.9600 val_auc=0.75524 val_acc=0.6527 (best 0.77041, patience 3/3)\n",
      "Early stopping: no improvement.\n",
      "\n",
      "=== Trial 5/6912 | idx=4 ===\n",
      "{'seq_len': 256, 'emb_dim': 160, 'conv_blocks': 2, 'channels_start': 128, 'channel_growth': 'x1.5', 'kernel_sizes': '3-5-7', 'dropout': 0.25, 'weight_decay': 0.0002, 'label_smoothing': 0.0, 'learning_rate': 0.0008, 'batch_size': 64, 'epochs': 8, 'loss_fn': 'bce_logits'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCRATCH] Epoch 1/8 | train_loss=1.1306 train_acc=0.5595 val_auc=0.73556 val_acc=0.6749 (best 0.73556, patience 0/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCRATCH] Epoch 2/8 | train_loss=0.6225 train_acc=0.6895 val_auc=0.74803 val_acc=0.6453 (best 0.74803, patience 0/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Trial interrupted by user. Saved heartbeat & latest checkpoint.\n",
      "\n",
      "=== Trial 6/6912 | idx=5 ===\n",
      "{'seq_len': 224, 'emb_dim': 192, 'conv_blocks': 2, 'channels_start': 128, 'channel_growth': 'x1.5', 'kernel_sizes': '3-5-7', 'dropout': 0.25, 'weight_decay': 0.0001, 'label_smoothing': 0.0, 'learning_rate': 0.001, 'batch_size': 128, 'epochs': 8, 'loss_fn': 'bce_logits'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCRATCH] Epoch 1/8 | train_loss=1.4604 train_acc=0.5465 val_auc=0.70917 val_acc=0.5567 (best 0.70917, patience 0/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCRATCH] Epoch 2/8 | train_loss=0.6969 train_acc=0.6642 val_auc=0.77692 val_acc=0.6798 (best 0.77692, patience 0/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCRATCH] Epoch 3/8 | train_loss=0.5040 train_acc=0.7554 val_auc=0.76808 val_acc=0.6601 (best 0.77692, patience 1/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCRATCH] Epoch 4/8 | train_loss=0.3588 train_acc=0.8398 val_auc=0.77114 val_acc=0.6576 (best 0.77692, patience 2/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCRATCH] Epoch 5/8 | train_loss=0.2404 train_acc=0.9125 val_auc=0.78396 val_acc=0.6946 (best 0.78396, patience 0/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCRATCH] Epoch 6/8 | train_loss=0.1993 train_acc=0.9217 val_auc=0.78367 val_acc=0.6872 (best 0.78396, patience 1/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCRATCH] Epoch 7/8 | train_loss=0.1542 train_acc=0.9526 val_auc=0.79284 val_acc=0.7069 (best 0.79284, patience 0/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCRATCH] Epoch 8/8 | train_loss=0.1270 train_acc=0.9618 val_auc=0.78723 val_acc=0.7118 (best 0.79284, patience 1/3)\n",
      "\n",
      "=== Trial 7/6912 | idx=6 ===\n",
      "{'seq_len': 200, 'emb_dim': 192, 'conv_blocks': 2, 'channels_start': 128, 'channel_growth': 'x1.5', 'kernel_sizes': '3-5-7', 'dropout': 0.2, 'weight_decay': 0.0001, 'label_smoothing': 0.03, 'learning_rate': 0.0008, 'batch_size': 64, 'epochs': 8, 'loss_fn': 'bce_logits'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/8:  46%|████▌     | 12/26 [00:01<00:01,  8.22it/s]"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Jigsaw — SCRATCH Hyperparameter Tuning (Grid/Random, Resumable)\n",
    "# * Preprocessing: nulls, normalization, outlier clipping\n",
    "# * TextCNN (BN + Dropout) from scratch; activation/residual options\n",
    "# * tqdm console bars (no ipywidgets), Windows-safe DataLoader\n",
    "# * Early stopping + TensorBoard (train loss/acc, val AUC/ACC, hparams)\n",
    "# * Saves per-trial artifacts + checkpoints; robust to interrupts\n",
    "# * Resume/skip via trial_results_scratch.csv; ALL remaining when max_trials=0\n",
    "# * Best trial writes submission_scratch.(csv|xlsx)\n",
    "# ============================================================\n",
    "\n",
    "import os, re, json, time, random, hashlib, platform, math, html\n",
    "from datetime import datetime, timezone\n",
    "from itertools import product\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "# ------------------- Paths & switches -------------------\n",
    "TRAIN_PATH = \"train.csv\"\n",
    "TEST_PATH  = \"test.csv\"\n",
    "SUB_PATH   = \"sample_submission.csv\"\n",
    "\n",
    "RESULTS_CSV = \"trial_results_scratch.csv\"    # resumable log (append)\n",
    "CHECKPOINT_DIR = \"checkpoints_scratch\"       # per-trial .pt files\n",
    "SAVE_CHECKPOINTS = True\n",
    "\n",
    "# TensorBoard\n",
    "ENABLE_TENSORBOARD = True                    # turn on/off\n",
    "TB_LOGDIR_BASE = \"tb_scratch_tune\"           # tensorboard --logdir tb_scratch_tune\n",
    "TB_WRITE_HPARAMS = True\n",
    "\n",
    "EARLY_STOP_PATIENCE = 3                      # epochs without AUC gain before stopping (0 to disable)\n",
    "MAX_TRIALS_PER_RUN  = 0                      # 0/None = run ALL remaining combos\n",
    "SAVE_BEST_SESSION_SUBMISSION = True\n",
    "SUBMISSION_CSV  = \"submission_scratch.csv\"\n",
    "SUBMISSION_XLSX = \"submission_scratch.xlsx\"\n",
    "\n",
    "# Save extra artifacts per trial\n",
    "SAVE_TRIAL_ARTIFACTS = True\n",
    "ARTIFACT_DIR = \"artifacts_scratch\"\n",
    "\n",
    "# Preprocessing config\n",
    "NORMALIZE_TEXT = True\n",
    "OUTLIER_CHAR_MAX = 4000   # clip very long texts (per column)\n",
    "REPORT_TOP_OUTLIERS = 3   # just to print a tiny summary\n",
    "\n",
    "# --- IO / dataloader runtime safety (Windows/Jupyter safe) ---\n",
    "IS_WINDOWS = (os.name == \"nt\")\n",
    "NUM_WORKERS = 0 if IS_WINDOWS else 2         # avoid multiprocessing on Windows\n",
    "PERSISTENT_WORKERS = False\n",
    "PIN_MEMORY = torch.cuda.is_available()\n",
    "LOG_EVERY_N = 50                              # fallback batch logging if tqdm unavailable\n",
    "\n",
    "# Progress bars: force console (no ipywidgets)\n",
    "FORCE_CONSOLE_TQDM = True\n",
    "if FORCE_CONSOLE_TQDM:\n",
    "    os.environ[\"TQDM_NOTEBOOK\"] = \"0\"\n",
    "    try:\n",
    "        from tqdm import tqdm  # console bar\n",
    "    except Exception:\n",
    "        tqdm = None\n",
    "else:\n",
    "    tqdm = None\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(TB_LOGDIR_BASE, exist_ok=True)\n",
    "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
    "\n",
    "# ------------------- Load data -------------------\n",
    "assert os.path.exists(TRAIN_PATH) and os.path.exists(TEST_PATH) and os.path.exists(SUB_PATH), \\\n",
    "    \"Place train.csv, test.csv, sample_submission.csv in the working directory.\"\n",
    "\n",
    "TEXT_COLS = ['body','rule','subreddit','positive_example_1','positive_example_2','negative_example_1','negative_example_2']\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df  = pd.read_csv(TEST_PATH)\n",
    "\n",
    "# --- Preprocessing: nulls -> empty, normalization, outlier clipping ---\n",
    "URL_PATTERN = re.compile(r\"(https?://\\S+|www\\.\\S+)\", re.IGNORECASE)\n",
    "TAG_PATTERN = re.compile(r\"<[^>]+>\")\n",
    "WS_PATTERN  = re.compile(r\"\\s+\")\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    if not isinstance(s, str): return \"\"\n",
    "    s = html.unescape(s)\n",
    "    s = re.sub(URL_PATTERN, \" URL \", s)\n",
    "    s = re.sub(TAG_PATTERN, \" \", s)\n",
    "    s = s.replace(\"&amp;\", \"&\").replace(\"&lt;\", \"<\").replace(\"&gt;\", \">\")\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[\\t\\r\\n]+\", \" \", s)\n",
    "    s = re.sub(WS_PATTERN, \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def normalize_and_clip_df(df: pd.DataFrame, label=\"train\"):\n",
    "    counts = {}\n",
    "    for c in TEXT_COLS:\n",
    "        if c in df.columns:\n",
    "            # Nulls -> \"\"\n",
    "            df[c] = df[c].fillna(\"\").astype(str)\n",
    "            if NORMALIZE_TEXT:\n",
    "                df[c] = df[c].map(clean_text)\n",
    "            # Outlier clipping by char length\n",
    "            lens = df[c].str.len()\n",
    "            over = (lens > OUTLIER_CHAR_MAX)\n",
    "            n_over = int(over.sum())\n",
    "            if n_over:\n",
    "                df.loc[over, c] = df.loc[over, c].str.slice(0, OUTLIER_CHAR_MAX)\n",
    "            counts[c] = n_over\n",
    "    # brief report\n",
    "    if any(counts.values()):\n",
    "        print(f\"[Preprocess] {label}: clipped long texts per column (>{OUTLIER_CHAR_MAX} chars):\", counts)\n",
    "    return df\n",
    "\n",
    "train_df = normalize_and_clip_df(train_df, \"train\")\n",
    "test_df  = normalize_and_clip_df(test_df,  \"test\")\n",
    "\n",
    "def build_input_template(row):\n",
    "    return \" [SEP] \".join([\n",
    "        f\"[COMMENT] {row['body']}\",\n",
    "        f\"[RULE] {row['rule']}\",\n",
    "        f\"[POS_EX_1] {row['positive_example_1']}\",\n",
    "        f\"[POS_EX_2] {row['positive_example_2']}\",\n",
    "        f\"[NEG_EX_1] {row['negative_example_1']}\",\n",
    "        f\"[NEG_EX_2] {row['negative_example_2']}\",\n",
    "        f\"[SUBREDDIT] r/{row['subreddit']}\"\n",
    "    ])\n",
    "\n",
    "if \"input_text\" not in train_df.columns:\n",
    "    train_df[\"input_text\"] = train_df.apply(build_input_template, axis=1)\n",
    "    test_df[\"input_text\"]  = test_df.apply(build_input_template, axis=1)\n",
    "\n",
    "# ------------------- Utils -------------------\n",
    "def set_seed(seed:int=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def now_iso(): return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def combo_key(params:Dict[str,Any])->str:\n",
    "    s = json.dumps({k:params[k] for k in sorted(params)}, sort_keys=True)\n",
    "    return hashlib.md5(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def load_done_keys(path: str) -> set:\n",
    "    if not os.path.exists(path):\n",
    "        return set()\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        if \"key\" not in df.columns:\n",
    "            return set()\n",
    "        # Only skip trials that completed successfully\n",
    "        if \"status\" in df.columns:\n",
    "            df = df[df[\"status\"].astype(str).str.startswith(\"ok\")]\n",
    "        return set(df[\"key\"].astype(str).tolist())\n",
    "    except Exception:\n",
    "        return set()\n",
    "\n",
    "def append_result_row(row:Dict[str,Any], path=RESULTS_CSV):\n",
    "    df = pd.DataFrame([row], columns=list(row.keys()))\n",
    "    if os.path.exists(path): df.to_csv(path, mode=\"a\", header=False, index=False)\n",
    "    else:                    df.to_csv(path, index=False)\n",
    "\n",
    "# Heartbeat + safe I/O helpers\n",
    "def write_json(obj, path):\n",
    "    tmp = path + \".tmp\"\n",
    "    with open(tmp, \"w\") as f:\n",
    "        json.dump(obj, f, indent=2)\n",
    "    os.replace(tmp, path)  # atomic\n",
    "\n",
    "def save_latest_checkpoint(key, model_state, params):\n",
    "    if not SAVE_CHECKPOINTS:\n",
    "        return\n",
    "    torch.save({\"state_dict\": model_state, \"params\": params},\n",
    "               os.path.join(CHECKPOINT_DIR, f\"{key}_latest.pt\"))\n",
    "\n",
    "# ------------------- Tokenizer/Vocab -------------------\n",
    "TOKEN_RE = re.compile(r\"[A-Za-z0-9_']+\")\n",
    "def tokenize(s): return TOKEN_RE.findall((s or \"\").lower())\n",
    "\n",
    "VOCAB_CACHE: Dict[int, Dict[str,int]] = {}\n",
    "def build_vocab(df:pd.DataFrame, vocab_size:int=30000)->Dict[str,int]:\n",
    "    if vocab_size in VOCAB_CACHE: return VOCAB_CACHE[vocab_size]\n",
    "    from collections import Counter\n",
    "    cnt = Counter()\n",
    "    for col in [\"body\",\"rule\"]:\n",
    "        for txt in df[col].tolist():\n",
    "            cnt.update(tokenize(txt))\n",
    "    vocab = {\"<pad>\":0, \"<unk>\":1}\n",
    "    for i,(tok,_) in enumerate(cnt.most_common(vocab_size-2), start=2):\n",
    "        vocab[tok] = i\n",
    "    VOCAB_CACHE[vocab_size] = vocab\n",
    "    return vocab\n",
    "\n",
    "def encode_text(s, vocab, max_len):\n",
    "    ids = [vocab.get(t,1) for t in tokenize(s)][:max_len]\n",
    "    if len(ids) < max_len: ids += [0]*(max_len-len(ids))\n",
    "    return np.array(ids, dtype=np.int64)\n",
    "\n",
    "class ScratchDataset(Dataset):\n",
    "    def __init__(self, df, vocab, seq_len, with_labels=True):\n",
    "        self.df=df.reset_index(drop=True); self.vocab=vocab; self.seq_len=seq_len; self.with_labels=with_labels\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, i):\n",
    "        r = self.df.loc[i]\n",
    "        half = self.seq_len//2\n",
    "        x = np.concatenate([encode_text(r[\"body\"], self.vocab, half),\n",
    "                            encode_text(r[\"rule\"], self.vocab, half)])\n",
    "        if self.with_labels:\n",
    "            y = int(r[\"rule_violation\"])\n",
    "            return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.float32)\n",
    "        return torch.tensor(x, dtype=torch.long)\n",
    "\n",
    "def make_dataloader(ds: Dataset, batch_size: int, shuffle: bool) -> DataLoader:\n",
    "    kwargs = dict(batch_size=batch_size, shuffle=shuffle, num_workers=NUM_WORKERS)\n",
    "    if NUM_WORKERS > 0:\n",
    "        kwargs[\"prefetch_factor\"] = 2\n",
    "        kwargs[\"persistent_workers\"] = PERSISTENT_WORKERS\n",
    "    if torch.cuda.is_available():\n",
    "        kwargs[\"pin_memory\"] = PIN_MEMORY\n",
    "    return DataLoader(ds, **kwargs)\n",
    "\n",
    "# ------------------- Model -------------------\n",
    "def parse_kernel_sizes(spec:str):\n",
    "    ks = []\n",
    "    for k in str(spec).split(\"-\"):\n",
    "        k = k.strip()\n",
    "        if k.isdigit(): ks.append(int(k))\n",
    "    return ks or [3,5]\n",
    "\n",
    "def channel_schedule(start:int, blocks:int, growth:str):\n",
    "    chs = [start]\n",
    "    for _ in range(1, blocks):\n",
    "        if growth == \"x1.5\": chs.append(int(round(chs[-1]*1.5)))\n",
    "        elif growth == \"x2\": chs.append(chs[-1]*2)\n",
    "        else:                chs.append(chs[-1])\n",
    "    return chs\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, conv_blocks, channels_start,\n",
    "                 channel_growth, kernel_sizes_spec, use_batchnorm=True,\n",
    "                 pooling=\"max\", dropout=0.2, activation=\"relu\", residual=False):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        Act = nn.ReLU if str(activation).lower()==\"relu\" else nn.GELU\n",
    "        self.act = Act()\n",
    "        self.residual = bool(residual)\n",
    "        ks = parse_kernel_sizes(kernel_sizes_spec)\n",
    "        chs = channel_schedule(channels_start, conv_blocks, channel_growth)\n",
    "        self.blocks = nn.ModuleList()\n",
    "        in_ch = emb_dim\n",
    "        for bi in range(conv_blocks):\n",
    "            k = ks[min(bi, len(ks)-1)]\n",
    "            out_ch = chs[bi]\n",
    "            conv = nn.Conv1d(in_ch, out_ch, kernel_size=k, padding=k//2)\n",
    "            bn   = nn.BatchNorm1d(out_ch) if use_batchnorm else nn.Identity()\n",
    "            self.blocks.append(nn.ModuleDict({\"conv\": conv, \"bn\": bn}))\n",
    "            in_ch = out_ch\n",
    "        self.pooling = pooling\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(in_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.emb(x).transpose(1,2)   # [B,E,L]\n",
    "        for blk in self.blocks:\n",
    "            z = blk[\"conv\"](h)\n",
    "            z = blk[\"bn\"](z)\n",
    "            z = self.act(z)\n",
    "            if self.residual and z.shape == h.shape:\n",
    "                h = z + h\n",
    "            else:\n",
    "                h = z\n",
    "        if self.pooling == \"avg\":\n",
    "            h = F.adaptive_avg_pool1d(h,1).squeeze(-1)\n",
    "        else:\n",
    "            h = F.adaptive_max_pool1d(h,1).squeeze(-1)\n",
    "        h = self.drop(h)\n",
    "        return self.fc(h).squeeze(-1)\n",
    "\n",
    "# ------------------- Loss/Optim/Val -------------------\n",
    "class BCEWithLS(nn.Module):\n",
    "    def __init__(self, smoothing=0.0): super().__init__(); self.s=smoothing\n",
    "    def forward(self, logits, targets):\n",
    "        if self.s>0: targets = targets*(1-self.s)+0.5*self.s\n",
    "        return F.binary_cross_entropy_with_logits(logits, targets)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, smoothing=0.0): super().__init__(); self.g=gamma; self.s=smoothing\n",
    "    def forward(self, logits, targets):\n",
    "        p = torch.sigmoid(logits)\n",
    "        if self.s>0: targets = targets*(1-self.s)+0.5*self.s\n",
    "        loss_pos = -targets * ((1-p)**self.g) * torch.log(torch.clamp(p, 1e-8, 1.0))\n",
    "        loss_neg = -(1-targets) * (p**self.g) * torch.log(torch.clamp(1-p, 1.0-1e-8))\n",
    "        return (loss_pos+loss_neg).mean()\n",
    "\n",
    "def get_loss(name, smoothing):\n",
    "    return FocalLoss(2.0, smoothing) if name==\"focal\" else BCEWithLS(smoothing)\n",
    "\n",
    "def make_optimizer(model, name, lr, weight_decay):\n",
    "    if name == \"adamw\": return torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif name == \"sgd\": return torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "    else: raise ValueError(f\"Unknown optimizer: {name}\")\n",
    "\n",
    "def _epoch_validate(model, dl, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    preds, ys = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb,yb in dl:\n",
    "            xb,yb = xb.to(device), yb.to(device)\n",
    "            p = torch.sigmoid(model(xb)).detach().cpu().numpy()\n",
    "            preds.append(p); ys.append(yb.detach().cpu().numpy())\n",
    "    preds = np.concatenate(preds); ys = np.concatenate(ys)\n",
    "    auc = roc_auc_score(ys, preds)\n",
    "    acc = accuracy_score(ys.astype(int), (preds >= 0.5).astype(int))\n",
    "    return auc, acc, preds, ys\n",
    "\n",
    "# ------------------- Train one combo -------------------\n",
    "def train_eval_once_with_best(params:dict, enable_tb:bool=False, key:str=None):\n",
    "    key = key or combo_key(params)\n",
    "    set_seed(int(params[\"seed\"]))\n",
    "    vocab = build_vocab(train_df, int(params[\"vocab_size\"]))\n",
    "    seq_len = int(params[\"seq_len\"])\n",
    "    tr, va = train_test_split(train_df, test_size=0.2, random_state=int(params[\"seed\"]),\n",
    "                              stratify=train_df[\"rule_violation\"])\n",
    "    ds_tr = ScratchDataset(tr, vocab, seq_len, True)\n",
    "    ds_va = ScratchDataset(va, vocab, seq_len, True)\n",
    "    dl_tr = make_dataloader(ds_tr, int(params[\"batch_size\"]), True)\n",
    "    dl_va = make_dataloader(ds_va, int(params[\"batch_size\"]), False)\n",
    "\n",
    "    model = TextCNN(\n",
    "        vocab_size=len(vocab),\n",
    "        emb_dim=int(params[\"emb_dim\"]),\n",
    "        conv_blocks=int(params[\"conv_blocks\"]),\n",
    "        channels_start=int(params[\"channels_start\"]),\n",
    "        channel_growth=str(params[\"channel_growth\"]),\n",
    "        kernel_sizes_spec=str(params[\"kernel_sizes\"]),\n",
    "        use_batchnorm=bool(params[\"use_batchnorm\"]),\n",
    "        pooling=str(params[\"pooling\"]),\n",
    "        dropout=float(params[\"dropout\"]),\n",
    "        activation=str(params[\"activation\"]),\n",
    "        residual=bool(params[\"residual\"]),\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    opt = make_optimizer(model, str(params[\"optimizer\"]), float(params[\"learning_rate\"]), float(params[\"weight_decay\"]))\n",
    "    loss_fn = get_loss(str(params[\"loss_fn\"]), float(params[\"label_smoothing\"]))\n",
    "    grad_clip = float(params[\"grad_clip\"])\n",
    "    epochs = int(params[\"epochs\"])\n",
    "\n",
    "    pos_weight = None\n",
    "    if str(params[\"class_weighting\"])==\"balanced\":\n",
    "        pos_weight = torch.tensor([(len(tr)-tr[\"rule_violation\"].sum())/(tr[\"rule_violation\"].sum()+1e-6)], device=DEVICE)\n",
    "\n",
    "    tb = None\n",
    "    if enable_tb:\n",
    "        try:\n",
    "            from torch.utils.tensorboard import SummaryWriter\n",
    "            tag = (\n",
    "                f\"emb{params['emb_dim']}_cb{params['conv_blocks']}_ch{params['channels_start']}\"\n",
    "                f\"_lr{params['learning_rate']}_bs{params['batch_size']}\"\n",
    "            )\n",
    "            tb_run_dir = os.path.join(TB_LOGDIR_BASE, f\"{tag}_{datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%S')}\")\n",
    "            tb = SummaryWriter(log_dir=tb_run_dir)\n",
    "            tb.add_text(\"hparams/json\", json.dumps(params, indent=2))\n",
    "        except Exception as e:\n",
    "            print(\"TensorBoard unavailable:\", e)\n",
    "            tb = None\n",
    "\n",
    "    best_auc, best_acc, best_state = -1.0, 0.0, None\n",
    "    global_step = 0\n",
    "    no_improve = 0\n",
    "    epoch_hist = []\n",
    "    heartbeat_path = os.path.join(ARTIFACT_DIR, f\"{key}_heartbeat.json\")\n",
    "\n",
    "    try:\n",
    "        for ep in range(epochs):\n",
    "            model.train()\n",
    "            iterator = dl_tr if tqdm is None else tqdm(dl_tr, leave=False, desc=f\"Epoch {ep+1}/{epochs}\")\n",
    "            running_loss, nb = 0.0, 0\n",
    "            train_correct, train_total = 0, 0\n",
    "\n",
    "            for i, (xb, yb) in enumerate(iterator):\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                opt.zero_grad()\n",
    "                logits = model(xb)\n",
    "                loss = (F.binary_cross_entropy_with_logits(logits, yb, pos_weight=pos_weight)\n",
    "                        if pos_weight is not None else loss_fn(logits, yb))\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "                opt.step()\n",
    "\n",
    "                # track train loss & accuracy\n",
    "                running_loss += float(loss.item()); nb += 1\n",
    "                with torch.no_grad():\n",
    "                    probs = torch.sigmoid(logits)\n",
    "                    preds = (probs >= 0.5).float()\n",
    "                    train_correct += int((preds == yb).sum().item())\n",
    "                    train_total   += int(yb.numel())\n",
    "\n",
    "                if tqdm is None and (i % LOG_EVERY_N == 0):\n",
    "                    print(f\"  batch {i:>4}/{len(dl_tr)}  loss={float(loss.item()):.4f}\")\n",
    "                if tb:\n",
    "                    tb.add_scalar(\"train/loss\", float(loss.item()), global_step)\n",
    "                global_step += 1\n",
    "\n",
    "            # ---- validation at epoch end ----\n",
    "            auc, acc, _, _ = _epoch_validate(model, dl_va, device=DEVICE)\n",
    "            train_acc = (train_correct / max(1, train_total))\n",
    "            epoch_hist.append({\"epoch\": ep+1,\n",
    "                               \"train_loss\": (running_loss/nb if nb else None),\n",
    "                               \"train_acc\": float(train_acc),\n",
    "                               \"val_auc\": float(auc), \"val_acc\": float(acc)})\n",
    "\n",
    "            improved = auc > best_auc + 1e-5\n",
    "            if improved:\n",
    "                best_auc, best_acc = auc, acc\n",
    "                best_state = {k: v.detach().cpu() for k,v in model.state_dict().items()}\n",
    "                no_improve = 0\n",
    "            else:\n",
    "                no_improve += 1\n",
    "\n",
    "            print(f\"[SCRATCH] Epoch {ep+1}/{epochs} | \"\n",
    "                  f\"train_loss={running_loss/nb:.4f} train_acc={train_acc:.4f} \"\n",
    "                  f\"val_auc={auc:.5f} val_acc={acc:.4f} \"\n",
    "                  f\"(best {best_auc:.5f}, patience {no_improve}/{EARLY_STOP_PATIENCE})\")\n",
    "            if tb:\n",
    "                tb.add_scalar(\"train/acc\", float(train_acc), ep)\n",
    "                tb.add_scalar(\"val/auc\", float(auc), ep)\n",
    "                tb.add_scalar(\"val/accuracy\", float(acc), ep)\n",
    "\n",
    "            # ---- heartbeat + latest checkpoint EVERY epoch ----\n",
    "            if SAVE_TRIAL_ARTIFACTS:\n",
    "                write_json({\n",
    "                    \"timestamp\": now_iso(),\n",
    "                    \"key\": key,\n",
    "                    \"params\": params,\n",
    "                    \"best_auc\": float(best_auc),\n",
    "                    \"best_acc\": float(best_acc),\n",
    "                    \"last_epoch\": ep+1,\n",
    "                    \"history_tail\": epoch_hist[-5:],\n",
    "                }, heartbeat_path)\n",
    "            save_latest_checkpoint(key, {k: v.detach().cpu() for k,v in model.state_dict().items()}, params)\n",
    "\n",
    "            if EARLY_STOP_PATIENCE and no_improve >= EARLY_STOP_PATIENCE:\n",
    "                print(\"Early stopping: no improvement.\")\n",
    "                break\n",
    "\n",
    "        # Final validation predictions/labels for artifacts\n",
    "        final_auc, final_acc, final_probs, final_true = _epoch_validate(model, dl_va, device=DEVICE)\n",
    "\n",
    "        if tb:\n",
    "            tb.add_scalar(\"val/best_auc\", float(best_auc))\n",
    "            tb.add_scalar(\"val/best_acc\", float(best_acc))\n",
    "            if TB_WRITE_HPARAMS:\n",
    "                try:\n",
    "                    from torch.utils.tensorboard.summary import hparams\n",
    "                    metric_dict = {\"hparam/best_auc\": float(best_auc), \"hparam/best_acc\": float(best_acc)}\n",
    "                    tb.file_writer.add_summary(hparams(params, metric_dict))\n",
    "                except Exception:\n",
    "                    pass\n",
    "            tb.close()\n",
    "\n",
    "        return (best_auc, best_acc, best_state, vocab,\n",
    "                epoch_hist, final_probs.tolist(), final_true.tolist())\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        # save a heartbeat right when interrupted\n",
    "        if SAVE_TRIAL_ARTIFACTS:\n",
    "            write_json({\n",
    "                \"timestamp\": now_iso(),\n",
    "                \"key\": key,\n",
    "                \"params\": params,\n",
    "                \"best_auc\": float(best_auc),\n",
    "                \"best_acc\": float(best_acc),\n",
    "                \"interrupted\": True,\n",
    "                \"history\": epoch_hist\n",
    "            }, heartbeat_path)\n",
    "        # also persist the latest weights\n",
    "        save_latest_checkpoint(key, {k: v.detach().cpu() for k,v in model.state_dict().items()}, params)\n",
    "        raise  # let caller log CSV row with \"interrupted\"\n",
    "\n",
    "# ------------------- Predict test with a state -------------------\n",
    "def predict_test_with_state(best_state, params, vocab, out_csv=\"submission_scratch.csv\"):\n",
    "    seq_len = int(params[\"seq_len\"])\n",
    "    class TestDS(Dataset):\n",
    "        def __init__(self, df, vocab, seq_len):\n",
    "            self.df=df.reset_index(drop=True); self.vocab=vocab; self.seq_len=seq_len\n",
    "        def __len__(self): return len(self.df)\n",
    "        def __getitem__(self, i):\n",
    "            r = self.df.loc[i]\n",
    "            half = self.seq_len//2\n",
    "            x = np.concatenate([encode_text(r[\"body\"], self.vocab, half),\n",
    "                                encode_text(r[\"rule\"], self.vocab, half)])\n",
    "            return torch.tensor(x, dtype=torch.long)\n",
    "\n",
    "    test_ds = TestDS(test_df, vocab, seq_len)\n",
    "    test_dl = make_dataloader(test_ds, int(params[\"batch_size\"]), False)\n",
    "\n",
    "    model = TextCNN(\n",
    "        vocab_size=len(vocab),\n",
    "        emb_dim=int(params[\"emb_dim\"]),\n",
    "        conv_blocks=int(params[\"conv_blocks\"]),\n",
    "        channels_start=int(params[\"channels_start\"]),\n",
    "        channel_growth=str(params[\"channel_growth\"]),\n",
    "        kernel_sizes_spec=str(params[\"kernel_sizes\"]),\n",
    "        use_batchnorm=bool(params[\"use_batchnorm\"]),\n",
    "        pooling=str(params[\"pooling\"]),\n",
    "        dropout=float(params[\"dropout\"]),\n",
    "        activation=str(params[\"activation\"]),\n",
    "        residual=bool(params[\"residual\"]),\n",
    "    ).to(DEVICE)\n",
    "    model.load_state_dict({k: v.to(DEVICE) for k,v in best_state.items()})\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for xb in test_dl:\n",
    "            xb = xb.to(DEVICE)\n",
    "            p = torch.sigmoid(model(xb)).detach().cpu().numpy()\n",
    "            preds.append(p)\n",
    "    preds = np.concatenate(preds).reshape(-1)\n",
    "\n",
    "    sub = pd.read_csv(SUB_PATH).copy()\n",
    "    if \"row_id\" not in sub.columns:\n",
    "        if \"row_id\" in test_df.columns:\n",
    "            sub = test_df[[\"row_id\"]].copy()\n",
    "        else:\n",
    "            sub[\"row_id\"] = np.arange(len(preds))\n",
    "    sub[\"rule_violation\"] = np.clip(preds, 0, 1)\n",
    "    sub.to_csv(out_csv, index=False)\n",
    "    print(f\"✅ Wrote {out_csv} (rows={len(sub)})\")\n",
    "    return out_csv\n",
    "\n",
    "# ------------------- Subgroup evaluation (optional interpretability) -------------------\n",
    "def evaluate_subgroups(params, best_state, vocab):\n",
    "    \"\"\"Optional: quick subgroup metrics by rule/subreddit on validation split.\"\"\"\n",
    "    set_seed(int(params[\"seed\"]))\n",
    "    seq_len = int(params[\"seq_len\"])\n",
    "    tr, va = train_test_split(train_df, test_size=0.2, random_state=int(params[\"seed\"]),\n",
    "                              stratify=train_df[\"rule_violation\"])\n",
    "    ds_va = ScratchDataset(va, vocab, seq_len, True)\n",
    "    dl_va = make_dataloader(ds_va, int(params[\"batch_size\"]), False)\n",
    "\n",
    "    model = TextCNN(\n",
    "        vocab_size=len(vocab),\n",
    "        emb_dim=int(params[\"emb_dim\"]),\n",
    "        conv_blocks=int(params[\"conv_blocks\"]),\n",
    "        channels_start=int(params[\"channels_start\"]),\n",
    "        channel_growth=str(params[\"channel_growth\"]),\n",
    "        kernel_sizes_spec=str(params[\"kernel_sizes\"]),\n",
    "        use_batchnorm=bool(params[\"use_batchnorm\"]),\n",
    "        pooling=str(params[\"pooling\"]),\n",
    "        dropout=float(params[\"dropout\"]),\n",
    "        activation=str(params[\"activation\"]),\n",
    "        residual=bool(params[\"residual\"]),\n",
    "    ).to(DEVICE)\n",
    "    model.load_state_dict({k: v.to(DEVICE) for k,v in best_state.items()})\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for xb,_ in dl_va:\n",
    "            xb = xb.to(DEVICE)\n",
    "            p = torch.sigmoid(model(xb)).detach().cpu().numpy()\n",
    "            preds.append(p)\n",
    "    preds = np.concatenate(preds).reshape(-1)\n",
    "\n",
    "    va_local = va.reset_index(drop=True).copy()\n",
    "    va_local[\"pred\"] = preds\n",
    "    va_local[\"pred_bin\"] = (va_local[\"pred\"] >= 0.5).astype(int)\n",
    "\n",
    "    def _agg(group):\n",
    "        y = group[\"rule_violation\"].values\n",
    "        p = group[\"pred\"].values\n",
    "        return pd.Series({\n",
    "            \"AUC\": roc_auc_score(y, p) if len(np.unique(y))>1 else np.nan,\n",
    "            \"ACC\": accuracy_score(y.astype(int), (p>=0.5).astype(int))\n",
    "        })\n",
    "    by_rule = va_local.groupby(\"rule\", dropna=False).apply(_agg)\n",
    "    by_sr   = va_local.groupby(\"subreddit\", dropna=False).apply(_agg).sort_values(\"AUC\", ascending=False).head(10)\n",
    "    print(\"\\n=== Interpretation ===\")\n",
    "    print(\"Top 10 subreddits by AUC:\\n\", by_sr)\n",
    "    print(\"\\nBy rule:\\n\", by_rule)\n",
    "    return by_rule, by_sr\n",
    "\n",
    "# ------------------- Param space handling -------------------\n",
    "CONSTANTS_DEFAULT = {\n",
    "    \"vocab_size\": 30000,\n",
    "    \"use_batchnorm\": True,\n",
    "    \"pooling\": \"max\",\n",
    "    \"optimizer\": \"adamw\",\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"scheduler\": \"none\",        # catalog only\n",
    "    \"class_weighting\": \"none\",\n",
    "    \"seed\": 42,\n",
    "    \"activation\": \"relu\",\n",
    "    \"residual\": False,\n",
    "}\n",
    "REQ = ['seq_len','emb_dim','conv_blocks','channels_start','channel_growth','kernel_sizes',\n",
    "       'dropout','weight_decay','label_smoothing','learning_rate','batch_size','epochs','loss_fn']\n",
    "INTS   = [\"seq_len\",\"emb_dim\",\"conv_blocks\",\"channels_start\",\"batch_size\",\"epochs\",\"seed\"]\n",
    "FLOATS = [\"dropout\",\"weight_decay\",\"label_smoothing\",\"learning_rate\",\"grad_clip\"]\n",
    "STRS   = [\"channel_growth\",\"kernel_sizes\",\"pooling\",\"optimizer\",\"scheduler\",\"loss_fn\",\"class_weighting\",\"activation\"]\n",
    "BOOLS  = [\"use_batchnorm\",\"residual\"]\n",
    "\n",
    "def coerce_one(p:Dict[str,Any])->Dict[str,Any]:\n",
    "    x = {**CONSTANTS_DEFAULT, **p}\n",
    "    missing = [k for k in REQ if k not in x]\n",
    "    if missing: raise KeyError(f\"Missing required param(s): {missing}\")\n",
    "    for k in INTS:   x[k] = int(x[k])\n",
    "    for k in FLOATS: x[k] = float(x[k])\n",
    "    for k in STRS:   x[k] = str(x[k])\n",
    "    for k in BOOLS:\n",
    "        v = x[k]; x[k] = (v.strip().lower() in (\"true\",\"1\",\"yes\",\"y\")) if isinstance(v,str) else bool(v)\n",
    "    return x\n",
    "\n",
    "def expand_grid(space:Dict[str,List[Any]], shuffle=True, seed=42)->List[Dict[str,Any]]:\n",
    "    keys = list(space.keys())\n",
    "    vals = [space[k] if isinstance(space[k], (list, tuple)) else [space[k]] for k in keys]\n",
    "    combos = []\n",
    "    for tup in product(*vals):\n",
    "        combos.append({k:v for k,v in zip(keys, tup)})\n",
    "    if shuffle:\n",
    "        rnd = random.Random(seed); rnd.shuffle(combos)\n",
    "    return combos\n",
    "\n",
    "# ------------------- Tuner (grid/random + resume + interrupt-safe) -------------------\n",
    "def run_param_space(space:Dict[str,List[Any]],\n",
    "                    constants:Dict[str,Any]=None,\n",
    "                    mode:str=\"grid\",        # \"grid\" or \"random\"\n",
    "                    n_samples:int=None,     # only for mode=\"random\"\n",
    "                    max_trials:int=10,\n",
    "                    enable_tb:bool=False,\n",
    "                    save_best_submission:bool=True):\n",
    "    constants = constants or {}\n",
    "    grid = expand_grid(space, shuffle=True, seed=int(constants.get(\"seed\", 42)))\n",
    "    if mode == \"random\" and n_samples is not None:\n",
    "        grid = grid[:n_samples]  # shuffled already\n",
    "\n",
    "    done = load_done_keys(RESULTS_CSV)\n",
    "    print(f\"Total combos: {len(grid)} | Completed in CSV: {len(done)}\")\n",
    "\n",
    "    # Allow unlimited runs if max_trials == 0/None\n",
    "    if max_trials in (None, 0):\n",
    "        max_trials = len(grid)\n",
    "\n",
    "    best_auc = -1.0\n",
    "    best_payload = None\n",
    "    ran = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    for idx, raw in enumerate(grid):\n",
    "        params = coerce_one({**raw, **constants})\n",
    "        key = combo_key(params)\n",
    "        if key in done:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n=== Trial {ran+1}/{max_trials} | idx={idx} ===\")\n",
    "        print({k: params[k] for k in REQ})\n",
    "\n",
    "        t1 = time.time()\n",
    "        running_marker = os.path.join(ARTIFACT_DIR, f\"{key}.running\")\n",
    "        open(running_marker, \"w\").close()  # create empty marker\n",
    "\n",
    "        auc = acc = np.nan\n",
    "        state = vocab = None\n",
    "        status = \"running\"\n",
    "        hist = []\n",
    "        final_probs = final_true = []\n",
    "\n",
    "        try:\n",
    "            (auc, acc, state, vocab,\n",
    "             hist, final_probs, final_true) = train_eval_once_with_best(params, enable_tb, key=key)\n",
    "            status = \"ok\"\n",
    "\n",
    "            if SAVE_CHECKPOINTS and state is not None:\n",
    "                torch.save({\"state_dict\": state, \"params\": params},\n",
    "                           os.path.join(CHECKPOINT_DIR, f\"{key}.pt\"))\n",
    "\n",
    "            if SAVE_TRIAL_ARTIFACTS:\n",
    "                write_json({\n",
    "                    \"timestamp\": now_iso(),\n",
    "                    \"params\": params,\n",
    "                    \"best_auc\": float(auc),\n",
    "                    \"best_acc\": float(acc),\n",
    "                    \"history\": hist\n",
    "                }, os.path.join(ARTIFACT_DIR, f\"{key}_metrics.json\"))\n",
    "                np.savez_compressed(os.path.join(ARTIFACT_DIR, f\"{key}_val.npz\"),\n",
    "                                    probs=np.array(final_probs, dtype=np.float32),\n",
    "                                    y=np.array(final_true, dtype=np.int64))\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            status = \"interrupted\"\n",
    "            print(\"⚠️ Trial interrupted by user. Saved heartbeat & latest checkpoint.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            status = f\"error: {e}\"\n",
    "            print(\"❌\", e)\n",
    "\n",
    "        finally:\n",
    "            dur = time.time() - t1\n",
    "            row_out = {\n",
    "                \"timestamp\": now_iso(),\n",
    "                \"key\": key,\n",
    "                \"mode\": \"scratch\",\n",
    "                \"device\": DEVICE,\n",
    "                \"python\": platform.python_version(),\n",
    "                \"grid_idx\": idx,\n",
    "                \"val_auc\": auc,\n",
    "                \"val_acc\": acc,\n",
    "                \"runtime_sec\": round(dur,2),\n",
    "                \"status\": status,\n",
    "                **{f\"hp/{k}\": params[k] for k in sorted(params)}\n",
    "            }\n",
    "            append_result_row(row_out, RESULTS_CSV)\n",
    "            ran += 1\n",
    "            try:\n",
    "                if os.path.exists(running_marker):\n",
    "                    os.remove(running_marker)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            if status == \"ok\" and auc > best_auc:\n",
    "                best_auc = auc\n",
    "                best_payload = (state, params, vocab)\n",
    "\n",
    "            if ran >= max_trials:\n",
    "                break\n",
    "\n",
    "    print(f\"\\nSession done. Ran {ran} trial(s) in {round(time.time()-t0,2)}s.\")\n",
    "    if best_payload and save_best_submission:\n",
    "        state, params, vocab = best_payload\n",
    "        predict_test_with_state(state, params, vocab, out_csv=SUBMISSION_CSV)\n",
    "        try:\n",
    "            sub_df = pd.read_csv(SUBMISSION_CSV)\n",
    "            with pd.ExcelWriter(SUBMISSION_XLSX, engine=\"xlsxwriter\") as w:\n",
    "                sub_df.to_excel(w, sheet_name=\"submission\", index=False)\n",
    "            print(f\"✅ Wrote {SUBMISSION_XLSX}\")\n",
    "        except Exception as e:\n",
    "            print(\"Note: could not write XLSX submission:\", e)\n",
    "    else:\n",
    "        print(\"No submission written this session.\")\n",
    "\n",
    "# ============================================================\n",
    "# DEFINE YOUR PARAM SPACE HERE (laptop-safe; resume lets you add more)\n",
    "# ============================================================\n",
    "PARAM_SPACE = dict(\n",
    "    # Capacity/structure\n",
    "    seq_len=[200, 224, 256],\n",
    "    emb_dim=[128, 160, 192],\n",
    "    conv_blocks=[1, 2],\n",
    "    channels_start=[128, 160],\n",
    "    channel_growth=[\"x1.5\"],\n",
    "    kernel_sizes=[\"3-5-7\"],\n",
    "    activation=[\"relu\", \"gelu\"],\n",
    "    residual=[False, True],\n",
    "\n",
    "    # Optimization/regularization\n",
    "    optimizer=[\"adamw\"],\n",
    "    learning_rate=[8e-4, 1e-3, 1.2e-3],\n",
    "    batch_size=[64, 128],\n",
    "    epochs=[8],\n",
    "    dropout=[0.2, 0.25],\n",
    "    weight_decay=[1e-4, 2e-4],\n",
    "    label_smoothing=[0.0, 0.03],\n",
    "    loss_fn=[\"bce_logits\"],\n",
    ")\n",
    "\n",
    "# Constants applied to every combo (change here if needed)\n",
    "CONSTANTS = dict(\n",
    "    vocab_size=30000,\n",
    "    use_batchnorm=True,\n",
    "    pooling=\"max\",\n",
    "    grad_clip=1.0,\n",
    "    scheduler=\"none\",\n",
    "    class_weighting=\"none\",\n",
    "    seed=42,\n",
    "    activation=\"relu\",\n",
    "    residual=False,\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# GO: run grid (or random sample) with resume\n",
    "# Start TensorBoard in a terminal:  tensorboard --logdir tb_scratch_tune\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    run_param_space(\n",
    "        PARAM_SPACE,\n",
    "        constants=CONSTANTS,\n",
    "        mode=\"grid\",             # or \"random\"\n",
    "        n_samples=None,          # only used for mode=\"random\"\n",
    "        max_trials=MAX_TRIALS_PER_RUN,  # 0/None => ALL remaining\n",
    "        enable_tb=ENABLE_TENSORBOARD,\n",
    "        save_best_submission=SAVE_BEST_SESSION_SUBMISSION\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3a2399",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
